{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f56db361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, argparse, gzip, os, pickle, warnings, copy, time\n",
    "import numpy as np, pytorch_lightning as pl\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a00a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = \"s2_mnist.gz\"\n",
    "MAX_EPOCHS = 20\n",
    "# MAX_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f450b1",
   "metadata": {},
   "source": [
    "Note that the validation is done with the test data!\n",
    "\n",
    "Also note what happens if I try to do early stopping with the test data directly: <br>\n",
    "RuntimeError: \"Early stopping conditioned on metric `test_acc` which is not available. Either add `test_acc` to the return of  validation_epoch end or modify your EarlyStopping callback to use any of the following: `val_loss`, `val_acc`\"\n",
    "\n",
    "The hparams are updated somewhere in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be50369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(pl.LightningModule):\n",
    "    def __init__(self, hparams, train_data, test_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.channels = hparams.channels\n",
    "        self.kernels = hparams.kernels\n",
    "        self.strides = hparams.strides\n",
    "        self.activation_fn = hparams.activation_fn\n",
    "        self.batch_norm = hparams.batch_norm\n",
    "        self.nodes = hparams.nodes\n",
    "        \n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        assert len(self.channels) == len(self.kernels) == len(self.strides)\n",
    "        possible_activation_fns = ['ReLU', 'LeakyReLU']\n",
    "        assert self.activation_fn in possible_activation_fns\n",
    "        \n",
    "        module_list = []\n",
    "        self.channels.insert(0,1)\n",
    "        \n",
    "        for i in range(len(self.channels)-1):\n",
    "            in_ch = self.channels[i]\n",
    "            out_ch = self.channels[i+1]\n",
    "            module_list.append(torch.nn.Conv2d(in_ch, out_ch, kernel_size=self.kernels[i], stride=self.strides[i]))\n",
    "            if self.activation_fn == 'ReLU':\n",
    "                module_list.append(torch.nn.ReLU())\n",
    "            elif self.activation_fn == 'LeakyReLU':\n",
    "                module_list.append(torch.nn.LeakyReLU())\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Activation function must be in {possible_activation_fns}.\")\n",
    "        \n",
    "        self.conv = torch.nn.Sequential(*module_list)\n",
    "        \n",
    "        \n",
    "        module_list = []\n",
    "        \n",
    "        self.nodes.insert(0,self.channels[-1])\n",
    "        self.nodes.append(10)\n",
    "        \n",
    "        for i in range(len(self.nodes) - 1):\n",
    "            in_nodes = self.nodes[i]\n",
    "            out_nodes = self.nodes[i+1]\n",
    "            if self.batch_norm:\n",
    "                module_list.append(torch.nn.BatchNorm1d(in_nodes))\n",
    "            module_list.append(torch.nn.Linear(in_features=in_nodes, out_features=out_nodes))\n",
    "            if i != (len(self.nodes) - 2):\n",
    "                if self.activation_fn == 'ReLU':\n",
    "                    module_list.append(torch.nn.ReLU())\n",
    "                elif self.activation_fn == 'LeakyReLU':\n",
    "                    module_list.append(torch.nn.LeakyReLU())\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"Activation function must be in {possible_activation_fns}.\")\n",
    "                \n",
    "        self.dense = torch.nn.Sequential(*module_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.shape[0], x.shape[1], -1)\n",
    "        x = torch.mean(x, dim=2)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, y_true):\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_function(y_pred, y_true)\n",
    "        return loss\n",
    "    \n",
    "    def correct_predictions(self, x, y_true):\n",
    "        outputs = self(x)\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "        correct = (y_pred == y_true).long().sum()\n",
    "        return correct\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset=self.train_data,\n",
    "                                           batch_size=self.hparams.train_batch_size,\n",
    "                                           shuffle=True, num_workers=self.hparams.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset=self.test_data,\n",
    "                                           batch_size=self.hparams.test_batch_size,\n",
    "                                           shuffle=False, num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset=self.test_data,\n",
    "                                           batch_size=self.hparams.test_batch_size,\n",
    "                                           shuffle=False, num_workers=self.hparams.num_workers)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        self._optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr,\n",
    "                                            weight_decay=self.hparams.weight_decay, amsgrad=False)\n",
    "        \n",
    "        return {'optimizer': self._optimizer}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.loss(x, y)\n",
    "#         mlflow.log_metric('train_loss', loss.item())\n",
    "#         self.log({'train_loss': loss.item()})\n",
    "        # add logs\n",
    "        logs = {'loss': loss.cpu().item()}\n",
    "        return {'loss': loss, 'log': logs}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().item()\n",
    "        return {'avg_train_loss': avg_loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.loss(x, y)\n",
    "        correct = self.correct_predictions(x, y)\n",
    "        return {'val_loss': loss, 'val_correct': correct}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean().cpu().item()\n",
    "        test_correct = torch.stack([x['val_correct'] for x in outputs]).sum().cpu()\n",
    "        test_acc = test_correct / len(self.test_data)\n",
    "\n",
    "        logs = {'val_loss': avg_loss, 'val_acc': test_acc}        \n",
    "        return {'val_loss': avg_loss, 'val_acc': test_acc, 'log': logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.loss(x, y)\n",
    "        correct = self.correct_predictions(x, y)\n",
    "        return {'test_loss': loss, 'test_correct': correct}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean().cpu().item()\n",
    "        test_correct = torch.stack([x['test_correct'] for x in outputs]).sum().cpu()\n",
    "        test_acc = test_correct / len(self.test_data)\n",
    "\n",
    "        logs = {'test_loss': avg_loss, 'test_acc': test_acc}        \n",
    "        return {'test_loss': avg_loss, 'test_acc': test_acc, 'log': logs}\n",
    "\n",
    "    def get_progress_bar_dict(self):\n",
    "        # call .item() only once but store elements without graphs\n",
    "        running_train_loss = self.trainer.running_loss.mean()\n",
    "        avg_training_loss = running_train_loss.cpu().item() if running_train_loss is not None else float('NaN')\n",
    "        lr = self.hparams.lr\n",
    "\n",
    "        tqdm_dict = {\n",
    "            'loss': '{:.2E}'.format(avg_training_loss),\n",
    "            'lr': '{:.2E}'.format(lr),\n",
    "        }\n",
    "\n",
    "        if self.trainer.truncated_bptt_steps is not None:\n",
    "            tqdm_dict['split_idx'] = self.trainer.split_idx\n",
    "\n",
    "        if self.trainer.logger is not None and self.trainer.logger.version is not None:\n",
    "            tqdm_dict['v_num'] = self.trainer.logger.version\n",
    "\n",
    "        return tqdm_dict\n",
    "\n",
    "    \n",
    "    def count_trainable_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b82991ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(path):\n",
    "    \n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        \n",
    "    train_data = torch.from_numpy(dataset[\"images\"][:, None, :, :].astype(np.float32))\n",
    "    train_labels = torch.from_numpy(dataset[\"labels\"].astype(np.int64))\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "    \n",
    "    return train_dataset\n",
    "    \n",
    "def load_test_data(path):\n",
    "    \n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        \n",
    "    test_data = torch.from_numpy(dataset[\"test\"][\"images\"][:, None, :, :].astype(np.float32))\n",
    "    test_labels = torch.from_numpy(dataset[\"test\"][\"labels\"].astype(np.int64))\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "    \n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7adf6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('GPU available: ' + torch.cuda.get_device_name())\n",
    "else:\n",
    "    raise RuntimeError('No GPU found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082086da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams114k = argparse.Namespace()\n",
    "\n",
    "hparams114k.name = '114k'\n",
    "hparams114k.train_batch_size = 32\n",
    "hparams114k.test_batch_size = 32\n",
    "hparams114k.num_workers = 0\n",
    "hparams114k.lr = 1e-3\n",
    "hparams114k.weight_decay = 0.\n",
    "\n",
    "hparams114k.channels = [13, 15, 22, 31, 141]\n",
    "hparams114k.kernels = [5, 3, 9, 7, 3]\n",
    "hparams114k.strides = [1, 1, 1, 1, 2]\n",
    "hparams114k.activation_fn = 'ReLU'\n",
    "hparams114k.batch_norm = True\n",
    "hparams114k.nodes = [64, 32]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hparams0_5M = argparse.Namespace()\n",
    "\n",
    "hparams0_5M.name = '0.5M'\n",
    "hparams0_5M.train_batch_size = 32\n",
    "hparams0_5M.test_batch_size = 32\n",
    "hparams0_5M.num_workers = 0\n",
    "hparams0_5M.lr = 1e-3\n",
    "hparams0_5M.weight_decay = 0.\n",
    "\n",
    "hparams0_5M.channels = [12, 13, 16, 77, 96, 163]\n",
    "hparams0_5M.kernels = [3, 3, 5, 5, 3, 5]\n",
    "hparams0_5M.strides = [1, 1, 1, 2, 1, 1]\n",
    "hparams0_5M.activation_fn = 'ReLU'\n",
    "hparams0_5M.batch_norm = True\n",
    "hparams0_5M.nodes = [64, 32]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hparams2_6M = argparse.Namespace()\n",
    "\n",
    "hparams2_6M.name = '2.6M'\n",
    "hparams2_6M.train_batch_size = 32\n",
    "hparams2_6M.test_batch_size = 32\n",
    "hparams2_6M.num_workers = 0\n",
    "hparams2_6M.lr = 1e-3\n",
    "hparams2_6M.weight_decay = 0.\n",
    "\n",
    "hparams2_6M.channels = [12, 15, 16, 85, 141, 191, 1100]\n",
    "hparams2_6M.kernels = [3, 5, 3, 7, 5, 3, 3]\n",
    "hparams2_6M.strides = [1, 1, 1, 2, 1, 1, 2]\n",
    "hparams2_6M.activation_fn = 'ReLU'\n",
    "hparams2_6M.batch_norm = True\n",
    "hparams2_6M.nodes = [64, 32]\n",
    "\n",
    "\n",
    "\n",
    "hyperparameters = [hparams114k, hparams0_5M, hparams2_6M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "275fc2da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(hparams, train_data, test_data):\n",
    "    \n",
    "    args_copy = copy.deepcopy(vars(hparams))\n",
    "    hparams1 = argparse.Namespace(**args_copy)\n",
    "    \n",
    "    model = ConvNet(hparams1, train_data, test_data)\n",
    "\n",
    "    print(f\"Number of trainable / total parameters: {model.count_trainable_parameters(), model.count_trainable_parameters()}\")\n",
    "\n",
    "    monitor = 'val_acc'\n",
    "    mode = 'max'\n",
    "    early_stopping = pl.callbacks.EarlyStopping(monitor=monitor, min_delta=0., patience=10, mode=mode)\n",
    "    checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(monitor=monitor, mode=mode)\n",
    "\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=MAX_EPOCHS, logger=False, early_stop_callback=early_stopping, checkpoint_callback=checkpoint)\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "#     trainer.test(model)\n",
    "\n",
    "    best_model = torch.load(checkpoint.best_model_path)\n",
    "    model.load_state_dict(best_model['state_dict'])\n",
    "    model.eval()\n",
    "    test_results = trainer.test(model)\n",
    "    \n",
    "    return test_results, copy.deepcopy(vars(hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6921e452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test examples: 10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918d5343cd8e4a14aac8d8c23de6e956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb258d596dc483a98247e8b7c7f3c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b15cc2b781410bb0915a724464b4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable / total parameters: (113761, 113761)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | loss_function | CrossEntropyLoss | 0     \n",
      "1 | conv          | Sequential       | 101 K \n",
      "2 | dense         | Sequential       | 11 K  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448023d355fa4abcbfd5acce8fc1e9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2677/3388982297.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0minner_dummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                     \u001b[0mtest_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresulting_hparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0minner_dummy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresulting_hparams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2677/1458788266.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(hparams, train_data, test_data)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#     trainer.test(model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/schuh/conda_envs/envs/s2cnn_j/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/schuh/conda_envs/envs/s2cnn_j/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/schuh/conda_envs/envs/s2cnn_j/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "test_data = load_test_data(TEST_PATH)\n",
    "\n",
    "print(\"Total test examples: {}\".format(len(test_data)))\n",
    "\n",
    "# training_set_sizes = [60000, 80000, 100000, 120000, 240000, 600000]\n",
    "training_set_sizes = [10000, 20000, 30000, 40000, 50000]\n",
    "outer_dummy = []\n",
    "\n",
    "with tqdm(total=len(hyperparameters)) as pbar:\n",
    "    for hparams in hyperparameters:\n",
    "        \n",
    "        middle_dummy = []\n",
    "        with tqdm(total=len(training_set_sizes)) as qbar:\n",
    "            for TRAIN_SAMPLES in training_set_sizes:\n",
    "                TRAIN_PATH = \"s2_mnist_train_dwr_\" + str(TRAIN_SAMPLES) + \".gz\"\n",
    "                train_data = load_train_data(TRAIN_PATH)\n",
    "                print(\"Total training examples: {}\".format(len(train_data)))\n",
    "\n",
    "                inner_dummy = []\n",
    "                for i in tqdm(range(3)):\n",
    "                    test_results, resulting_hparams = train_model(hparams, train_data, test_data)\n",
    "\n",
    "                    inner_dummy.append([test_results, resulting_hparams])\n",
    "\n",
    "                middle_dummy.append(inner_dummy)\n",
    "                qbar.update(1)\n",
    "\n",
    "        outer_dummy.append(middle_dummy)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'first_tests_smaller_training_sets.pickle'\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    filename = str(time.time()) + filename\n",
    "    print('File already existed, timestamp was prepended to filename.')\n",
    "    \n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(outer_dummy, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416243b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
