{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "489b0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from s2cnn import SO3Convolution\n",
    "from s2cnn import S2Convolution\n",
    "from s2cnn import so3_integrate\n",
    "from s2cnn import so3_near_identity_grid\n",
    "from s2cnn import s2_near_identity_grid\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7ade8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_PATH = \"s2_mnist.gz\"\n",
    "MODEL_NAME = \"s2cnn\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-3\n",
    "\n",
    "# network = 'original'\n",
    "network = 'deep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b0fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, batch_size):\n",
    "\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    train_data = torch.from_numpy(\n",
    "        dataset[\"train\"][\"images\"][:, None, :, :].astype(np.float32))\n",
    "    train_labels = torch.from_numpy(\n",
    "        dataset[\"train\"][\"labels\"].astype(np.int64))\n",
    "\n",
    "    # TODO normalize dataset\n",
    "    # mean = train_data.mean()\n",
    "    # stdv = train_data.std()\n",
    "\n",
    "    train_dataset = data_utils.TensorDataset(train_data, train_labels)\n",
    "    train_loader = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_data = torch.from_numpy(\n",
    "        dataset[\"test\"][\"images\"][:, None, :, :].astype(np.float32))\n",
    "    test_labels = torch.from_numpy(\n",
    "        dataset[\"test\"][\"labels\"].astype(np.int64))\n",
    "\n",
    "    test_dataset = data_utils.TensorDataset(test_data, test_labels)\n",
    "    test_loader = data_utils.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader, train_dataset, test_dataset\n",
    "\n",
    "\n",
    "class S2ConvNet_original(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(S2ConvNet_original, self).__init__()\n",
    "\n",
    "        f1 = 20\n",
    "        f2 = 40\n",
    "        f_output = 10\n",
    "\n",
    "        b_in = 30\n",
    "        b_l1 = 10\n",
    "        b_l2 = 6\n",
    "\n",
    "        grid_s2 = s2_near_identity_grid()\n",
    "        grid_so3 = so3_near_identity_grid()\n",
    "\n",
    "        self.conv1 = S2Convolution(\n",
    "            nfeature_in=1,\n",
    "            nfeature_out=f1,\n",
    "            b_in=b_in,\n",
    "            b_out=b_l1,\n",
    "            grid=grid_s2)\n",
    "\n",
    "        self.conv2 = SO3Convolution(\n",
    "            nfeature_in=f1,\n",
    "            nfeature_out=f2,\n",
    "            b_in=b_l1,\n",
    "            b_out=b_l2,\n",
    "            grid=grid_so3)\n",
    "\n",
    "        self.out_layer = nn.Linear(f2, f_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = so3_integrate(x)\n",
    "\n",
    "        x = self.out_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class S2ConvNet_deep(nn.Module):\n",
    "\n",
    "    def __init__(self, bandwidth=30):\n",
    "        super(S2ConvNet_deep, self).__init__()\n",
    "\n",
    "        grid_s2    =  s2_near_identity_grid(n_alpha=6, max_beta=np.pi/16, n_beta=1)\n",
    "        grid_so3_1 = so3_near_identity_grid(n_alpha=6, max_beta=np.pi/16, n_beta=1, max_gamma=2*np.pi, n_gamma=6)\n",
    "        grid_so3_2 = so3_near_identity_grid(n_alpha=6, max_beta=np.pi/ 8, n_beta=1, max_gamma=2*np.pi, n_gamma=6)\n",
    "        grid_so3_3 = so3_near_identity_grid(n_alpha=6, max_beta=np.pi/ 4, n_beta=1, max_gamma=2*np.pi, n_gamma=6)\n",
    "        grid_so3_4 = so3_near_identity_grid(n_alpha=6, max_beta=np.pi/ 2, n_beta=1, max_gamma=2*np.pi, n_gamma=6)\n",
    "\n",
    "        self.convolutional = nn.Sequential(\n",
    "            S2Convolution(\n",
    "                nfeature_in  = 1,\n",
    "                nfeature_out = 8,\n",
    "                b_in  = bandwidth,\n",
    "                b_out = bandwidth,\n",
    "                grid=grid_s2),\n",
    "            nn.ReLU(inplace=False),\n",
    "            SO3Convolution(\n",
    "                nfeature_in  =  8,\n",
    "                nfeature_out = 16,\n",
    "                b_in  = bandwidth,\n",
    "                b_out = bandwidth//2,\n",
    "                grid=grid_so3_1),\n",
    "            nn.ReLU(inplace=False),\n",
    "\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 16,\n",
    "                nfeature_out = 16,\n",
    "                b_in  = bandwidth//2,\n",
    "                b_out = bandwidth//2,\n",
    "                grid=grid_so3_2),\n",
    "            nn.ReLU(inplace=False),\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 16,\n",
    "                nfeature_out = 24,\n",
    "                b_in  = bandwidth//2,\n",
    "                b_out = bandwidth//4,\n",
    "                grid=grid_so3_2),\n",
    "            nn.ReLU(inplace=False),\n",
    "\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 24,\n",
    "                nfeature_out = 24,\n",
    "                b_in  = bandwidth//4,\n",
    "                b_out = bandwidth//4,\n",
    "                grid=grid_so3_3),\n",
    "            nn.ReLU(inplace=False),\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 24,\n",
    "                nfeature_out = 32,\n",
    "                b_in  = bandwidth//4,\n",
    "                b_out = bandwidth//8,\n",
    "                grid=grid_so3_3),\n",
    "            nn.ReLU(inplace=False),\n",
    "\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 32,\n",
    "                nfeature_out = 64,\n",
    "                b_in  = bandwidth//8,\n",
    "                b_out = bandwidth//8,\n",
    "                grid=grid_so3_4),\n",
    "            nn.ReLU(inplace=False)\n",
    "            )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            # linear 1\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(in_features=64,out_features=64),\n",
    "            nn.ReLU(inplace=False),\n",
    "            # linear 2\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.ReLU(inplace=False),\n",
    "            # linear 3\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Linear(in_features=32, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional(x)\n",
    "        x = so3_integrate(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ca891f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#params 156882\n",
      "load 0.pkl.gz... done\n",
      "load 0.pkl.gz... done\n",
      "load 1.pkl.gz... done\n",
      "load 2.pkl.gz... done\n",
      "load 1.pkl.gz... done\n",
      "load 3.pkl.gz... done\n",
      "load 4.pkl.gz... done\n",
      "load 2.pkl.gz... done\n",
      "load 5.pkl.gz... done\n",
      "load 3.pkl.gz... done\n",
      "load 6.pkl.gz... done\n",
      "load 7.pkl.gz... done\n",
      "load 4.pkl.gz... done\n",
      "load 8.pkl.gz... done\n",
      "load 5.pkl.gz... done\n",
      "load 9.pkl.gz... done\n",
      "load 10.pkl.gz... done\n",
      "load 6.pkl.gz... done\n",
      "Epoch [1/20], Iter [167/1875] Loss: 0.6264"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5858/3779730715.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         print('\\rEpoch [{0}/{1}], Iter [{2}/{3}] Loss: {4:.4f}'.format(\n\u001b[1;32m     33\u001b[0m             \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             loss.item()), end=\"\")\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, train_dataset, _ = load_data(MNIST_PATH, BATCH_SIZE)\n",
    "\n",
    "if network == 'original':\n",
    "    classifier = S2ConvNet_original()\n",
    "elif network == 'deep':\n",
    "    classifier = S2ConvNet_deep()\n",
    "else:\n",
    "    raise ValueError('Unknown network architecture')\n",
    "classifier.to(DEVICE)\n",
    "\n",
    "print(\"#params\", sum(x.numel() for x in classifier.parameters()))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        classifier.train()\n",
    "\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        print('\\rEpoch [{0}/{1}], Iter [{2}/{3}] Loss: {4:.4f}'.format(\n",
    "            epoch+1, NUM_EPOCHS, i+1, len(train_dataset)//BATCH_SIZE,\n",
    "            loss.item()), end=\"\")\n",
    "    print(\"\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "\n",
    "        classifier.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            outputs = classifier(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).long().sum().item()\n",
    "\n",
    "    print('Test Accuracy: {0}'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb284d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), MODEL_NAME + '_' + network + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57569b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
