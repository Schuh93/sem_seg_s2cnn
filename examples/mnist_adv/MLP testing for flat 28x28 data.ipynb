{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0d03435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, argparse, warnings, copy\n",
    "import pytorch_lightning as pl\n",
    "from tqdm.notebook import tqdm\n",
    "from data_loader import load_train_data, load_test_data\n",
    "from models import CConvNet, S2ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8a7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMPLES = 20000\n",
    "# MAX_EPOCHS = 20\n",
    "MAX_EPOCHS = 3\n",
    "# MIN_DELTA = 0.\n",
    "# PATIENCE = 10\n",
    "\n",
    "TRAIN_PATH = \"flat_mnist_train_28x28_\" + str(TRAIN_SAMPLES) + \".gz\"\n",
    "TEST_PATH = \"flat_mnist_test_28x28.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7544e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_train_data(TRAIN_PATH), load_train_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231845a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 20000\n",
      "Total test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Total training examples: {}\".format(len(train_data)))\n",
    "print(\"Total test examples: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95d9047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = argparse.Namespace()\n",
    "\n",
    "hparams.name = 'test_model'\n",
    "hparams.train_batch_size = 32\n",
    "hparams.test_batch_size = 32\n",
    "hparams.num_workers = 0\n",
    "hparams.lr = 1e-3\n",
    "hparams.weight_decay = 0.\n",
    "\n",
    "hparams.channels = [16, 24, 32, 64]\n",
    "hparams.kernels = [3, 3, 3, 3]\n",
    "hparams.strides = [1, 1, 1, 1]\n",
    "hparams.activation_fn = 'LeakyReLU'\n",
    "hparams.batch_norm = True\n",
    "hparams.nodes = [64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c01afb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "model = CConvNet(hparams, train_data, test_data)\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "31b8257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35970"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dfc5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175fd4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac7f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = 's2_mnist_cs1.gz'\n",
    "path2 = 'datasets/s2_mnist_test_sphere_center.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1, data2 = load_test_data(path1), load_train_data(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a029a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[0][0].unsqueeze(0)\n",
    "xs = x.size()\n",
    "print(xs)\n",
    "x = x.reshape(xs[0], -1)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff35da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, hparams, train_data, test_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hparams = copy.deepcopy(hparams)\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        self.activation_fn = self.hparams.activation_fn\n",
    "        self.batch_norm = self.hparams.batch_norm\n",
    "        self.nodes = self.hparams.nodes.copy()\n",
    "        \n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        possible_activation_fns = ['ReLU', 'LeakyReLU']\n",
    "        assert self.activation_fn in possible_activation_fns\n",
    "\n",
    "        module_list = []\n",
    "        \n",
    "        self.nodes.insert(0,28*28)\n",
    "        self.nodes.append(10)\n",
    "        \n",
    "        for i in range(len(self.nodes) - 1):\n",
    "            in_nodes = self.nodes[i]\n",
    "            out_nodes = self.nodes[i+1]\n",
    "            if self.batch_norm:\n",
    "                if i>0:\n",
    "                    module_list.append(torch.nn.BatchNorm1d(in_nodes))\n",
    "            module_list.append(torch.nn.Linear(in_features=in_nodes, out_features=out_nodes))\n",
    "            if i != (len(self.nodes) - 2):\n",
    "                if self.activation_fn == 'ReLU':\n",
    "                    module_list.append(torch.nn.ReLU())\n",
    "                elif self.activation_fn == 'LeakyReLU':\n",
    "                    module_list.append(torch.nn.LeakyReLU())\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"Activation function must be in {possible_activation_fns}.\")\n",
    "                \n",
    "        self.dense = torch.nn.Sequential(*module_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xs = x.size()\n",
    "        x = x.reshape(xs[0], -1)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, y_true):\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_function(y_pred, y_true)\n",
    "        return loss\n",
    "    \n",
    "    def correct_predictions(self, x, y_true):\n",
    "        outputs = self(x)\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "        correct = (y_pred == y_true).long().sum()\n",
    "        return correct\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset=self.train_data,\n",
    "                                           batch_size=self.hparams.train_batch_size,\n",
    "                                           shuffle=True, num_workers=self.hparams.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset=self.test_data,\n",
    "                                           batch_size=self.hparams.test_batch_size,\n",
    "                                           shuffle=False, num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset=self.test_data,\n",
    "                                           batch_size=self.hparams.test_batch_size,\n",
    "                                           shuffle=False, num_workers=self.hparams.num_workers)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        self._optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr,\n",
    "                                            weight_decay=self.hparams.weight_decay, amsgrad=False)\n",
    "        \n",
    "        return {'optimizer': self._optimizer}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.loss(x, y)\n",
    "        correct = self.correct_predictions(x, y)\n",
    "        \n",
    "        logs = {'loss': loss.cpu().item()}\n",
    "        return {'loss': loss, 'train_correct': correct, 'log': logs}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().item()\n",
    "        train_correct = torch.stack([x['train_correct'] for x in outputs]).sum().cpu()\n",
    "        train_acc = train_correct / len(self.train_data)\n",
    "        \n",
    "        logs = {'train_loss': avg_loss, 'train_acc': train_acc}    \n",
    "        return {'train_loss': avg_loss, 'train_acc': train_acc, 'log': logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.loss(x, y)\n",
    "        correct = self.correct_predictions(x, y)\n",
    "        return {'val_loss': loss, 'val_correct': correct}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean().cpu().item()\n",
    "        val_correct = torch.stack([x['val_correct'] for x in outputs]).sum().cpu()\n",
    "        val_acc = val_correct / len(self.test_data)\n",
    "\n",
    "        logs = {'val_loss': avg_loss, 'val_acc': val_acc}        \n",
    "        return {'val_loss': avg_loss, 'val_acc': val_acc, 'log': logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.loss(x, y)\n",
    "        correct = self.correct_predictions(x, y)\n",
    "        return {'test_loss': loss, 'test_correct': correct}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean().cpu().item()\n",
    "        test_correct = torch.stack([x['test_correct'] for x in outputs]).sum().cpu()\n",
    "        test_acc = test_correct / len(self.test_data)\n",
    "\n",
    "        logs = {'test_loss': avg_loss, 'test_acc': test_acc}        \n",
    "        return {'test_loss': avg_loss, 'test_acc': test_acc, 'log': logs}\n",
    "\n",
    "    def get_progress_bar_dict(self):\n",
    "        running_train_loss = self.trainer.running_loss.mean()\n",
    "        avg_training_loss = running_train_loss.cpu().item() if running_train_loss is not None else float('NaN')\n",
    "        lr = self.hparams.lr\n",
    "\n",
    "        tqdm_dict = {\n",
    "            'loss': '{:.2E}'.format(avg_training_loss),\n",
    "            'lr': '{:.2E}'.format(lr),\n",
    "        }\n",
    "\n",
    "        if self.trainer.truncated_bptt_steps is not None:\n",
    "            tqdm_dict['split_idx'] = self.trainer.split_idx\n",
    "\n",
    "        if self.trainer.logger is not None and self.trainer.logger.version is not None:\n",
    "            tqdm_dict['v_num'] = self.trainer.logger.version\n",
    "\n",
    "        return tqdm_dict\n",
    "\n",
    "    \n",
    "    def count_trainable_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "30f640d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_hparams = argparse.Namespace()\n",
    "\n",
    "MLP_hparams.name = 'test_model'\n",
    "MLP_hparams.train_batch_size = 32\n",
    "MLP_hparams.test_batch_size = 32\n",
    "MLP_hparams.num_workers = 0\n",
    "MLP_hparams.lr = 1e-3\n",
    "MLP_hparams.weight_decay = 0.\n",
    "\n",
    "MLP_hparams.activation_fn = 'LeakyReLU'\n",
    "MLP_hparams.batch_norm = True\n",
    "# MLP_hparams.nodes = [44, 23]\n",
    "MLP_hparams.nodes = [500, 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5b0321b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model = MLP(MLP_hparams, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8b715f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470460"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_model.count_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa2bfdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (loss_function): CrossEntropyLoss()\n",
       "  (dense): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=42, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm1d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=42, out_features=22, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm1d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=22, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f9f83a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_hparams = argparse.Namespace()\n",
    "\n",
    "S2_hparams.name = 'test_model'\n",
    "S2_hparams.train_batch_size = 32\n",
    "S2_hparams.test_batch_size = 32\n",
    "S2_hparams.num_workers = 0\n",
    "S2_hparams.lr = 1e-4\n",
    "S2_hparams.weight_decay = 0.\n",
    "\n",
    "S2_hparams.channels = [8, 11, 64]\n",
    "S2_hparams.bandlimit = [14, 8, 2]\n",
    "S2_hparams.kernel_max_beta = [0.05, 0.125, 0.5]\n",
    "S2_hparams.activation_fn = 'LeakyReLU'\n",
    "S2_hparams.batch_norm = True\n",
    "S2_hparams.nodes = [64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6531d8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35533"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2_model = S2ConvNet(S2_hparams, train_data, test_data)\n",
    "S2_model.count_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c744c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
