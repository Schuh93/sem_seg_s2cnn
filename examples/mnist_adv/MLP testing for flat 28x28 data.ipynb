{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d03435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, argparse, warnings, copy\n",
    "import pytorch_lightning as pl\n",
    "from tqdm.notebook import tqdm\n",
    "from data_loader import load_train_data, load_test_data\n",
    "from models import CConvNet, S2ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8a7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMPLES = 20000\n",
    "# MAX_EPOCHS = 20\n",
    "MAX_EPOCHS = 3\n",
    "# MIN_DELTA = 0.\n",
    "# PATIENCE = 10\n",
    "\n",
    "TRAIN_PATH = \"flat_mnist_train_28x28_\" + str(TRAIN_SAMPLES) + \".gz\"\n",
    "TEST_PATH = \"flat_mnist_test_28x28.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7544e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_train_data(TRAIN_PATH), load_train_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231845a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 20000\n",
      "Total test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Total training examples: {}\".format(len(train_data)))\n",
    "print(\"Total test examples: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d9047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = argparse.Namespace()\n",
    "\n",
    "hparams.name = 'test_model'\n",
    "hparams.train_batch_size = 32\n",
    "hparams.test_batch_size = 32\n",
    "hparams.num_workers = 0\n",
    "hparams.lr = 1e-3\n",
    "hparams.weight_decay = 0.\n",
    "\n",
    "hparams.channels = [16, 24, 32, 64]\n",
    "hparams.kernels = [3, 3, 3, 3]\n",
    "hparams.strides = [1, 1, 1, 1]\n",
    "hparams.activation_fn = 'LeakyReLU'\n",
    "hparams.batch_norm = True\n",
    "hparams.nodes = [64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01afb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "model = CConvNet(hparams, train_data, test_data)\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=MAX_EPOCHS, logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31b8257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35970"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24dfc5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | loss_function | CrossEntropyLoss | 0     \n",
      "1 | conv          | Sequential       | 29 K  \n",
      "2 | dense         | Sequential       | 6 K   \n",
      "/media/data/schuh/conda_envs/envs/s2cnn_j/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/schuh/conda_envs/envs/s2cnn_j/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4ec9b6ba224a04809ac0618a917d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/schuh/conda_envs/envs/s2cnn_j/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: The metric you returned 0.1576978862285614 must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of loss in validation_epoch_end()?\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/schuh/conda_envs/envs/s2cnn_j/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: The metric you returned 0.18812403082847595 must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of loss in validation_epoch_end()?\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/schuh/conda_envs/envs/s2cnn_j/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: The metric you returned 0.0391991101205349 must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of loss in validation_epoch_end()?\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "175fd4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/schuh/conda_envs/envs/s2cnn_j/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9e696124554b9d8b1bd89f88220bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'test_acc': tensor(0.9667), 'test_loss': 0.10872550308704376}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.10872550308704376, 'test_acc': 0.96670001745224}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff35da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, hparams, train_data, test_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hparams = copy.deepcopy(hparams)\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        self.activation_fn = self.hparams.activation_fn\n",
    "        self.batch_norm = self.hparams.batch_norm\n",
    "        self.nodes = self.hparams.nodes.copy()\n",
    "        \n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        possible_activation_fns = ['ReLU', 'LeakyReLU']\n",
    "        assert self.activation_fn in possible_activation_fns\n",
    "\n",
    "        module_list = []\n",
    "        \n",
    "        self.nodes.insert(0,28*28)\n",
    "        self.nodes.append(10)\n",
    "        \n",
    "        for i in range(len(self.nodes) - 1):\n",
    "            in_nodes = self.nodes[i]\n",
    "            out_nodes = self.nodes[i+1]\n",
    "            if self.batch_norm:\n",
    "                if i>0:\n",
    "                    module_list.append(torch.nn.BatchNorm1d(in_nodes))\n",
    "            module_list.append(torch.nn.Linear(in_features=in_nodes, out_features=out_nodes))\n",
    "            if i != (len(self.nodes) - 2):\n",
    "                if self.activation_fn == 'ReLU':\n",
    "                    module_list.append(torch.nn.ReLU())\n",
    "                elif self.activation_fn == 'LeakyReLU':\n",
    "                    module_list.append(torch.nn.LeakyReLU())\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"Activation function must be in {possible_activation_fns}.\")\n",
    "                \n",
    "        self.dense = torch.nn.Sequential(*module_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xs = x.size()\n",
    "        x = x.reshape(xs[0], -1)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, y_true):\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_function(y_pred, y_true)\n",
    "        return loss\n",
    "    \n",
    "    def correct_predictions(self, x, y_true):\n",
    "        outputs = self(x)\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "        correct = (y_pred == y_true).long().sum()\n",
    "        return correct\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset=self.train_data,\n",
    "                                           batch_size=self.hparams.train_batch_size,\n",
    "                                           shuffle=True, num_workers=self.hparams.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset=self.test_data,\n",
    "                                           batch_size=self.hparams.test_batch_size,\n",
    "                                           shuffle=False, num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset=self.test_data,\n",
    "                                           batch_size=self.hparams.test_batch_size,\n",
    "                                           shuffle=False, num_workers=self.hparams.num_workers)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        self._optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr,\n",
    "                                            weight_decay=self.hparams.weight_decay, amsgrad=False)\n",
    "        \n",
    "        return {'optimizer': self._optimizer}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.loss(x, y)\n",
    "        correct = self.correct_predictions(x, y)\n",
    "        \n",
    "        logs = {'loss': loss.cpu().item()}\n",
    "        return {'loss': loss, 'train_correct': correct, 'log': logs}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().item()\n",
    "        train_correct = torch.stack([x['train_correct'] for x in outputs]).sum().cpu()\n",
    "        train_acc = train_correct / len(self.train_data)\n",
    "        \n",
    "        logs = {'train_loss': avg_loss, 'train_acc': train_acc}    \n",
    "        return {'train_loss': avg_loss, 'train_acc': train_acc, 'log': logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.loss(x, y)\n",
    "        correct = self.correct_predictions(x, y)\n",
    "        return {'val_loss': loss, 'val_correct': correct}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean().cpu().item()\n",
    "        val_correct = torch.stack([x['val_correct'] for x in outputs]).sum().cpu()\n",
    "        val_acc = val_correct / len(self.test_data)\n",
    "\n",
    "        logs = {'val_loss': avg_loss, 'val_acc': val_acc}        \n",
    "        return {'val_loss': avg_loss, 'val_acc': val_acc, 'log': logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.loss(x, y)\n",
    "        correct = self.correct_predictions(x, y)\n",
    "        return {'test_loss': loss, 'test_correct': correct}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean().cpu().item()\n",
    "        test_correct = torch.stack([x['test_correct'] for x in outputs]).sum().cpu()\n",
    "        test_acc = test_correct / len(self.test_data)\n",
    "\n",
    "        logs = {'test_loss': avg_loss, 'test_acc': test_acc}        \n",
    "        return {'test_loss': avg_loss, 'test_acc': test_acc, 'log': logs}\n",
    "\n",
    "    def get_progress_bar_dict(self):\n",
    "        running_train_loss = self.trainer.running_loss.mean()\n",
    "        avg_training_loss = running_train_loss.cpu().item() if running_train_loss is not None else float('NaN')\n",
    "        lr = self.hparams.lr\n",
    "\n",
    "        tqdm_dict = {\n",
    "            'loss': '{:.2E}'.format(avg_training_loss),\n",
    "            'lr': '{:.2E}'.format(lr),\n",
    "        }\n",
    "\n",
    "        if self.trainer.truncated_bptt_steps is not None:\n",
    "            tqdm_dict['split_idx'] = self.trainer.split_idx\n",
    "\n",
    "        if self.trainer.logger is not None and self.trainer.logger.version is not None:\n",
    "            tqdm_dict['v_num'] = self.trainer.logger.version\n",
    "\n",
    "        return tqdm_dict\n",
    "\n",
    "    \n",
    "    def count_trainable_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30f640d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_hparams = argparse.Namespace()\n",
    "\n",
    "MLP_hparams.name = 'test_model'\n",
    "MLP_hparams.train_batch_size = 32\n",
    "MLP_hparams.test_batch_size = 32\n",
    "MLP_hparams.num_workers = 0\n",
    "MLP_hparams.lr = 1e-3\n",
    "MLP_hparams.weight_decay = 0.\n",
    "\n",
    "MLP_hparams.activation_fn = 'LeakyReLU'\n",
    "MLP_hparams.batch_norm = True\n",
    "MLP_hparams.nodes = [44, 23]\n",
    "# MLP_hparams.nodes = [500, 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b0321b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model = MLP(MLP_hparams, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b715f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35949"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_model.count_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa2bfdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (loss_function): CrossEntropyLoss()\n",
       "  (dense): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=44, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm1d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=44, out_features=23, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=23, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fac9ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | loss_function | CrossEntropyLoss | 0     \n",
      "1 | dense         | Sequential       | 35 K  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454fc3b6b6db41b0b7fd559a4d978360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 624it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e69ecf18dc34eca946990170424e879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'test_acc': tensor(0.9351), 'test_loss': 0.22609630227088928}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.22609630227088928, 'test_acc': 0.9351000189781189}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(MLP_model)\n",
    "trainer.test(MLP_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9f83a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_hparams = argparse.Namespace()\n",
    "\n",
    "S2_hparams.name = 'test_model'\n",
    "S2_hparams.train_batch_size = 32\n",
    "S2_hparams.test_batch_size = 32\n",
    "S2_hparams.num_workers = 0\n",
    "S2_hparams.lr = 1e-4\n",
    "S2_hparams.weight_decay = 0.\n",
    "S2_hparams.image_size = 28\n",
    "\n",
    "S2_hparams.channels = [8, 11, 64]\n",
    "# S2_hparams.bandlimit = [12, 6, 2]\n",
    "S2_hparams.bandlimit = [14, 8,2]\n",
    "# S2_hparams.kernel_max_beta = [1/12, 1/6, 0.5]\n",
    "S2_hparams.kernel_max_beta = [1/14, 1/8, 0.5]\n",
    "S2_hparams.activation_fn = 'LeakyReLU'\n",
    "S2_hparams.batch_norm = True\n",
    "S2_hparams.nodes = [64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6531d8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35533"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2_model = S2ConvNet(S2_hparams, train_data, test_data)\n",
    "S2_model.count_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1d0b0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2ConvNet(\n",
       "  (loss_function): CrossEntropyLoss()\n",
       "  (conv): Sequential(\n",
       "    (0): S2Convolution(nfeature_in=1, nfeature_out=8, b_in=14, b_out=14, kernel_max_beta=0.0714)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): SO3Convolution(nfeature_in=8, nfeature_out=11, b_in=14, b_out=8, kernel_max_beta=0.1250)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): SO3Convolution(nfeature_in=11, nfeature_out=64, b_in=8, b_out=2, kernel_max_beta=0.5000)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (dense): Sequential(\n",
       "    (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0c744c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | loss_function | CrossEntropyLoss | 0     \n",
      "1 | conv          | Sequential       | 28 K  \n",
      "2 | dense         | Sequential       | 6 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a51e13b9884d35b575128e7b9b4c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 624it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bc03967983466bad3bd40dce68a178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'test_acc': tensor(0.6287), 'test_loss': 1.3620548248291016}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.3620548248291016, 'test_acc': 0.6287000179290771}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(S2_model)\n",
    "trainer.test(S2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf9b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
