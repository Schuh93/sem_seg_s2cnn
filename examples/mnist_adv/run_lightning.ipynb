{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1ca13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, argparse, gzip, os, pickle, warnings, copy, time\n",
    "import numpy as np, pytorch_lightning as pl\n",
    "from tqdm.notebook import tqdm\n",
    "from s2cnn import s2_near_identity_grid, so3_near_identity_grid, SO3Convolution, S2Convolution, so3_integrate\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59bac2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = \"s2_mnist.gz\"\n",
    "MAX_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09ec911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2ConvNet(pl.LightningModule):\n",
    "    def __init__(self, hparams, train_data, test_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.channels = hparams.channels\n",
    "        self.bandlimit = hparams.bandlimit\n",
    "        self.kernel_max_beta = hparams.kernel_max_beta\n",
    "        self.activation_fn = hparams.activation_fn\n",
    "        self.batch_norm = hparams.batch_norm\n",
    "        self.nodes = hparams.nodes\n",
    "        \n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        if isinstance(self.kernel_max_beta, float):\n",
    "            self.kernel_max_beta = [self.kernel_max_beta] * (len(self.channels))\n",
    "\n",
    "        assert len(self.channels) == len(self.bandlimit) == len(self.kernel_max_beta)\n",
    "        possible_activation_fns = ['ReLU', 'LeakyReLU']\n",
    "        assert self.activation_fn in possible_activation_fns\n",
    "        \n",
    "        \n",
    "        grid_s2 = s2_near_identity_grid(max_beta=self.kernel_max_beta[0] * np.pi, n_alpha=6, n_beta=1)\n",
    "        grids_so3 = [\n",
    "            so3_near_identity_grid(max_beta=max_beta * np.pi, n_alpha=6, n_beta=1, n_gamma=6) for max_beta in self.kernel_max_beta[1:]\n",
    "        ]\n",
    "        \n",
    "        module_list = []\n",
    "        self.channels.insert(0,1) # greyscale\n",
    "        self.bandlimit.insert(0,30) # depends on image size\n",
    "        \n",
    "        in_ch = self.channels[0]\n",
    "        out_ch = self.channels[1]\n",
    "        b_in = self.bandlimit[0]\n",
    "        b_out = self.bandlimit[1]\n",
    "        \n",
    "        module_list.append(S2Convolution(\n",
    "            nfeature_in=in_ch, nfeature_out=out_ch, b_in=b_in, b_out=b_out, grid=grid_s2\n",
    "        ))\n",
    "        \n",
    "        if self.activation_fn == 'ReLU':\n",
    "                module_list.append(torch.nn.ReLU())\n",
    "        elif self.activation_fn == 'LeakyReLU':\n",
    "            module_list.append(torch.nn.LeakyReLU())\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Activation function must be in {possible_activation_fns}.\")\n",
    "        \n",
    "        for i in range(1, len(self.channels)-1):\n",
    "            in_ch = self.channels[i]\n",
    "            out_ch = self.channels[i+1]\n",
    "            b_in = self.bandlimit[i]\n",
    "            b_out = self.bandlimit[i+1]\n",
    "            \n",
    "            module_list.append(\n",
    "                SO3Convolution(\n",
    "                    nfeature_in=in_ch,\n",
    "                    nfeature_out=out_ch,\n",
    "                    b_in=b_in,\n",
    "                    b_out=b_out,\n",
    "                    grid=grids_so3[i-1],\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            if self.activation_fn == 'ReLU':\n",
    "                module_list.append(torch.nn.ReLU())\n",
    "            elif self.activation_fn == 'LeakyReLU':\n",
    "                module_list.append(torch.nn.LeakyReLU())\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Activation function must be in {possible_activation_fns}.\")\n",
    "            \n",
    "        self.conv = torch.nn.Sequential(*module_list)\n",
    "        \n",
    "        \n",
    "        module_list = []\n",
    "        \n",
    "        self.nodes.insert(0,self.channels[-1])\n",
    "        self.nodes.append(10)\n",
    "        \n",
    "        for i in range(len(self.nodes) - 1):\n",
    "            in_nodes = self.nodes[i]\n",
    "            out_nodes = self.nodes[i+1]\n",
    "            if self.batch_norm:\n",
    "                module_list.append(torch.nn.BatchNorm1d(in_nodes))\n",
    "            module_list.append(torch.nn.Linear(in_features=in_nodes, out_features=out_nodes))\n",
    "            if i != (len(self.nodes) - 2):\n",
    "                if self.activation_fn == 'ReLU':\n",
    "                    module_list.append(torch.nn.ReLU())\n",
    "                elif self.activation_fn == 'LeakyReLU':\n",
    "                    module_list.append(torch.nn.LeakyReLU())\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"Activation function must be in {possible_activation_fns}.\")\n",
    "                \n",
    "        self.dense = torch.nn.Sequential(*module_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = so3_integrate(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, y_true):\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_function(y_pred, y_true)\n",
    "        return loss\n",
    "    \n",
    "    def correct_predictions(self, x, y_true):\n",
    "        outputs = self(x)\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "        correct = (y_pred == y_true).long().sum()\n",
    "        return correct\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset=self.train_data,\n",
    "                                           batch_size=self.hparams.train_batch_size,\n",
    "                                           shuffle=True, num_workers=self.hparams.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset=self.test_data,\n",
    "                                           batch_size=self.hparams.test_batch_size,\n",
    "                                           shuffle=False, num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset=self.test_data,\n",
    "                                           batch_size=self.hparams.test_batch_size,\n",
    "                                           shuffle=False, num_workers=self.hparams.num_workers)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        self._optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr,\n",
    "                                            weight_decay=self.hparams.weight_decay, amsgrad=False)\n",
    "        \n",
    "        return {'optimizer': self._optimizer}\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.loss(x, y)\n",
    "        logs = {'loss': loss.cpu().item()}\n",
    "        return {'loss': loss, 'log': logs}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().item()\n",
    "        return {'avg_train_loss': avg_loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.loss(x, y)\n",
    "        correct = self.correct_predictions(x, y)\n",
    "        return {'val_loss': loss, 'val_correct': correct}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean().cpu().item()\n",
    "        test_correct = torch.stack([x['val_correct'] for x in outputs]).sum().cpu()\n",
    "        test_acc = test_correct / len(self.test_data)\n",
    "\n",
    "        logs = {'val_loss': avg_loss, 'val_acc': test_acc}        \n",
    "        return {'val_loss': avg_loss, 'val_acc': test_acc, 'log': logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.loss(x, y)\n",
    "        correct = self.correct_predictions(x, y)\n",
    "        return {'test_loss': loss, 'test_correct': correct}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean().cpu().item()\n",
    "        test_correct = torch.stack([x['test_correct'] for x in outputs]).sum().cpu()\n",
    "        test_acc = test_correct / len(self.test_data)\n",
    "\n",
    "        logs = {'test_loss': avg_loss, 'test_acc': test_acc}        \n",
    "        return {'test_loss': avg_loss, 'test_acc': test_acc, 'log': logs}\n",
    "\n",
    "    def get_progress_bar_dict(self):\n",
    "        # call .item() only once but store elements without graphs\n",
    "        running_train_loss = self.trainer.running_loss.mean()\n",
    "        avg_training_loss = running_train_loss.cpu().item() if running_train_loss is not None else float('NaN')\n",
    "        lr = self.hparams.lr\n",
    "\n",
    "        tqdm_dict = {\n",
    "            'loss': '{:.2E}'.format(avg_training_loss),\n",
    "            'lr': '{:.2E}'.format(lr),\n",
    "        }\n",
    "\n",
    "        if self.trainer.truncated_bptt_steps is not None:\n",
    "            tqdm_dict['split_idx'] = self.trainer.split_idx\n",
    "\n",
    "        if self.trainer.logger is not None and self.trainer.logger.version is not None:\n",
    "            tqdm_dict['v_num'] = self.trainer.logger.version\n",
    "\n",
    "        return tqdm_dict\n",
    "    \n",
    "    def count_trainable_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c971b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(path):\n",
    "    \n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        \n",
    "    train_data = torch.from_numpy(dataset[\"images\"][:, None, :, :].astype(np.float32))\n",
    "    train_labels = torch.from_numpy(dataset[\"labels\"].astype(np.int64))\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "    \n",
    "    return train_dataset\n",
    "    \n",
    "def load_test_data(path):\n",
    "    \n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        \n",
    "    test_data = torch.from_numpy(dataset[\"test\"][\"images\"][:, None, :, :].astype(np.float32))\n",
    "    test_labels = torch.from_numpy(dataset[\"test\"][\"labels\"].astype(np.int64))\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "    \n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3092530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('GPU available: ' + torch.cuda.get_device_name())\n",
    "else:\n",
    "    raise RuntimeError('No GPU found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "decd5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = argparse.Namespace()\n",
    "\n",
    "hparams.name = '157k'\n",
    "hparams.train_batch_size = 32\n",
    "hparams.test_batch_size = 32\n",
    "hparams.num_workers = 0\n",
    "hparams.lr = 1e-4\n",
    "hparams.weight_decay = 0.\n",
    "\n",
    "hparams.channels = [8, 16, 16, 24, 24, 32, 64]\n",
    "hparams.bandlimit = [30, 15, 15, 8, 8, 4, 2]\n",
    "hparams.kernel_max_beta = [0.0625, 0.0625, 0.125, 0.125, 0.25, 0.25, 0.5]\n",
    "hparams.activation_fn = 'ReLU'\n",
    "hparams.batch_norm = True\n",
    "hparams.nodes = [64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a75a712",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(hparams, train_data, test_data):\n",
    "    \n",
    "    args_copy = copy.deepcopy(vars(hparams))\n",
    "    hparams1 = argparse.Namespace(**args_copy)\n",
    "    \n",
    "    model = S2ConvNet(hparams1, train_data, test_data)\n",
    "\n",
    "    print(f\"Number of trainable / total parameters: {model.count_trainable_parameters(), model.count_trainable_parameters()}\")\n",
    "\n",
    "    monitor = 'val_acc'\n",
    "    mode = 'max'\n",
    "    early_stopping = pl.callbacks.EarlyStopping(monitor=monitor, min_delta=0., patience=10, mode=mode)\n",
    "    checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(monitor=monitor, mode=mode)\n",
    "\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=MAX_EPOCHS, logger=False, early_stop_callback=early_stopping, checkpoint_callback=checkpoint)\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "    best_model = torch.load(checkpoint.best_model_path)\n",
    "    model.load_state_dict(best_model['state_dict'])\n",
    "    model.eval()\n",
    "    test_results = trainer.test(model)\n",
    "    \n",
    "    return test_results, copy.deepcopy(vars(hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984741f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test examples: 10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77aa7c7fd8b40999f4777b6e3150199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb18f2f202c04f1ab646bbcabb2607cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable / total parameters: (156882, 156882)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | loss_function | CrossEntropyLoss | 0     \n",
      "1 | conv          | Sequential       | 149 K \n",
      "2 | dense         | Sequential       | 6 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 0.pkl.gz... done\n",
      "load 0.pkl.gz... done\n",
      "load 1.pkl.gz... done\n",
      "load 2.pkl.gz... done\n",
      "load 1.pkl.gz... done\n",
      "load 3.pkl.gz... done\n",
      "load 4.pkl.gz... done\n",
      "load 2.pkl.gz... done\n",
      "load 14.pkl.gz... done\n",
      "compute 14.pkl.gz... save 14.pkl.gz... done\n",
      "load 15.pkl.gz... done\n",
      "load 16.pkl.gz... done\n",
      "compute 15.pkl.gz... save 15.pkl.gz... done\n",
      "load 17.pkl.gz... done\n",
      "compute 16.pkl.gz... save 16.pkl.gz... done\n",
      "load 18.pkl.gz... done\n",
      "load 19.pkl.gz... done\n",
      "compute 17.pkl.gz... save 17.pkl.gz... done\n",
      "load 20.pkl.gz... done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4203d24eecd745f380c4f285be3711d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = load_test_data(TEST_PATH)\n",
    "\n",
    "print(\"Total test examples: {}\".format(len(test_data)))\n",
    "\n",
    "training_set_sizes = [10000, 20000, 30000, 40000, 50000]\n",
    "\n",
    "middle_dummy = []\n",
    "with tqdm(total=len(training_set_sizes)) as qbar:\n",
    "    for TRAIN_SAMPLES in training_set_sizes:\n",
    "        TRAIN_PATH = \"s2_mnist_train_dwr_\" + str(TRAIN_SAMPLES) + \".gz\"\n",
    "        train_data = load_train_data(TRAIN_PATH)\n",
    "        print(\"Total training examples: {}\".format(len(train_data)))\n",
    "\n",
    "        inner_dummy = []\n",
    "        for i in tqdm(range(3)):\n",
    "            test_results, resulting_hparams = train_model(hparams, train_data, test_data)\n",
    "\n",
    "            inner_dummy.append([test_results, resulting_hparams])\n",
    "\n",
    "        middle_dummy.append(inner_dummy)\n",
    "        qbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe84c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'S2CNN_first_tests_2_smaller_training_sets.pickle'\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    filename = str(time.time()) + filename\n",
    "    print('File already existed, timestamp was prepended to filename.')\n",
    "    \n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(middle_dummy, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
